{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a530ee",
   "metadata": {},
   "source": [
    "# Dataset creation for sign spotting\n",
    "Here, we preprocess the data from the Corpus NGT and split it up by its annotations. The data corresponding to an annotation are then sampled so they are of a fixed length. \n",
    "\n",
    "If the target fixed length is X frames, we can check if an annotation is already X frames long. If so, we can simply add its (preprocessed) data as a new entry in the train, validation or test set. If instead the length of the annotation is less than X frames, we add zero padding. \n",
    "\n",
    "If the annotation is longer than X frames, we undersample the frames. For instance, assuming we have a target length X=10 and an annotation with 22 frames, we first drop every other (every second) frame to get to 11 frames. We do not use random undersampling here to ensure at least some of the temporal structure of the annotation is maintained. Then, we drop 1 more frame randomly to get to our target of 10 frames.\n",
    "\n",
    "We first create the dataset using the above method, before repeating the 'dataset creation' for the purposes of checking the correlation between the used features. The repeat is needed because for the correlation analysis, we do not want to  undersampling and zero padding be done, we simply use the annotations at their original length. After preparing for the correlation analysis, we use masking of the NaN values to not skew the correlation (e.g. by replacing NaNs with zeros). Lastly, we find out which features have correlation above some threshold so we can discard them before training our sign spotting model.\n",
    "\n",
    "**Note:** this notebook should be run *twice*, once with `ling_features = True` and once with `ling_features = False`. See the code block after the imports for where to change this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e0d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pympi\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import importlib\n",
    "\n",
    "# Keep python tools up to date\n",
    "from tools import tools, constants, make_dataset\n",
    "importlib.reload(tools)\n",
    "importlib.reload(constants)\n",
    "importlib.reload(make_dataset)\n",
    "\n",
    "# Import all functions from the tools\n",
    "from tools.tools import load_dict\n",
    "from tools.constants import PATHS, USE_MOUTHINGS # Path constants\n",
    "from tools.make_dataset import*\n",
    "\n",
    "np.random.seed(123) # Set random seed for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d679dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to use the features of the mouth or not\n",
    "mouthings = USE_MOUTHINGS\n",
    "mouthings_str = '' if not mouthings else '_with_mouthings'\n",
    "mouthing_ind = [13,14] # Mouthing indices (in Mediapipe's Face model)\n",
    "\n",
    "# If we should normalize or not\n",
    "normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6866b8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations...\n"
     ]
    }
   ],
   "source": [
    "# Root where all the annotated .eaf sign files are present\n",
    "dataset_root = PATHS['cngt_vids_and_eaf']\n",
    "\n",
    "# Path to a file of anns that only distinguishes between handedness\n",
    "# We want to make sure two-handed signs are added once as an annotation and not twice\n",
    "dataset_anns_path = PATHS['dataset_anns']\n",
    "\n",
    "# List the .eaf files in the root directory to investigate\n",
    "anns_in_dir = [file for file in os.listdir(dataset_root) if file.endswith('.eaf')]\n",
    "\n",
    "# Loading the annotations for the dataset-tailored annotations\n",
    "# Or we create them if they don't exist yet\n",
    "if os.path.exists(dataset_anns_path):\n",
    "    print('Loading annotations...')\n",
    "    anns_with_tiers = load_dict(dataset_anns_path)\n",
    "else:\n",
    "    print('Making annotations without manual simultaneity...')\n",
    "    anns_with_tiers = {}\n",
    "    for i, ann_file in enumerate(anns_in_dir):\n",
    "        print(i, end = '\\r')\n",
    "        # Read in the Eaf file \n",
    "        eaf_file = pympi.Elan.Eaf(os.path.join(dataset_root, ann_file))\n",
    "\n",
    "        # Get the glosses and mouthings of the file\n",
    "        anns_dict, _ = get_gloss_vals(eaf_file, True)\n",
    "        # As explained above, we only distinguish handedness and not simultaneity\n",
    "        anns_dict = man_sim_and_hand_dist(anns_dict, manual_sim = False)\n",
    "\n",
    "        # Store the glosses, mouthings and tiers\n",
    "        anns_with_tiers[ann_file] = anns_dict\n",
    "    print('Storing...')\n",
    "    with open(dataset_anns_path, 'wb') as f:\n",
    "        pickle.dump(anns_with_tiers, f)\n",
    "        \n",
    "# Signbank dictionary info\n",
    "df = pd.read_csv(PATHS['signbank_with_linguistics'])\n",
    "\n",
    "# Dictionary which contains which videos (and signers) belong in test/train set\n",
    "id_split = load_dict(PATHS['CNGT_split_ids'])\n",
    "\n",
    "# Using only the top X signs if we have a list of them\n",
    "top = -1 #1000\n",
    "top_signs_path = PATHS['top_signs'].format(top)\n",
    "if os.path.exists(top_signs_path):\n",
    "    top_signs = np.load(top_signs_path)\n",
    "else:\n",
    "    top_signs = []\n",
    "    \n",
    "top_suffix = '_top' if len(top_signs) != 0 else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466cf900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signer split across train, validation and test sets\n",
      "Train ['S001', 'S002', 'S003', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S010', 'S013', 'S014', 'S015', 'S016', 'S017', 'S018', 'S021', 'S022', 'S023', 'S024', 'S025', 'S026', 'S029', 'S030', 'S031', 'S032', 'S033', 'S034', 'S035', 'S036', 'S037', 'S038', 'S043', 'S044', 'S045', 'S049', 'S050', 'S059', 'S060', 'S061', 'S062', 'S063', 'S064', 'S065', 'S066', 'S067', 'S068', 'S069', 'S070', 'S075', 'S076', 'S077', 'S078', 'S083', 'S084', 'S087', 'S088', 'S089', 'S090', 'S091', 'S092']\n",
      "Val ['S011', 'S012', 'S019', 'S020', 'S027', 'S028', 'S039', 'S040', 'S041', 'S045', 'S046', 'S047', 'S048', 'S051', 'S052', 'S055', 'S056', 'S058', 'S071', 'S072', 'S073', 'S074', 'S079', 'S080', 'S081', 'S082', 'S085', 'S086']\n",
      "Test ['S011', 'S012', 'S019', 'S020', 'S027', 'S028', 'S039', 'S040', 'S041', 'S042', 'S045', 'S046', 'S047', 'S048', 'S051', 'S052', 'S053', 'S054', 'S055', 'S056', 'S057', 'S058', 'S071', 'S072', 'S073', 'S074', 'S080', 'S081', 'S082', 'S085', 'S086']\n"
     ]
    }
   ],
   "source": [
    "print('Signer split across train, validation and test sets')\n",
    "# Printing which signers belong in the train, validation and test set\n",
    "for key in id_split:\n",
    "    print(key, sorted(set([x.split('_')[-1] for x in id_split[key]])))\n",
    "# Note that the val and test sets are allowed to have the same signers (same Sxxx)\n",
    "# But that they should contain different videos\n",
    "val_vids = id_split['Val']\n",
    "test_vids = id_split['Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97ee55",
   "metadata": {},
   "source": [
    "# Data preprocessing and feature extraction\n",
    "**Note:** `extract_features` can take a while to run with `linguistic_features = True` due to the preprocessing and extraction for each video being computationally heavy. With `linguistic_features = False` this should only take a few minutes.\n",
    "\n",
    "*Expected runtime: 15-30min (with linguistic_features = True)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10272186",
   "metadata": {},
   "source": [
    "# Creating the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07eea9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CNGT files when split into numpy files: 4275\n",
      "Feature-extracted data exists, loading...\n",
      "Done!\n",
      "CPU times: total: 2.61 s\n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Extract the linguistic features\n",
    "ling_features, ling_mirrored = extract_features(anns_with_tiers, linguistic_features = True, mouthing_ind = mouthing_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "727732be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 855/855 [01:36<00:00,  8.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from X_train: [[ 0.46017638  0.40214708  0.9784996  ... -0.01298058  1.002342\n",
      "   0.15156126]\n",
      " [ 0.29058886  0.19711134  0.8826425  ... -0.00848746  1.0038315\n",
      "   0.13282895]\n",
      " [ 0.23168114  0.16862081  0.9759804  ...  0.00137353  1.0396366\n",
      "   0.12343681]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean annotation length 8.0\n",
      "\n",
      "Number of empty anns: 0\n",
      "Number of annotations: 110435\n",
      "62.58% <= 10 frames, 5.16%>= than 10 frames, 32.26% = 10 frames\n",
      "(10295,) (9585,)\n",
      "Unique glosses: 2712\n",
      "Mean shape: (120,) std shape: (120,)\n",
      "\tdataset \t\t\tdata shape \t\t label shape\n",
      "Train                    \t(180432, 10, 120)   \t\t(180432,)\n",
      "Train (no mirror)        \t(90216, 10, 120)    \t\t(90216,)\n",
      "Validation               \t(10295, 10, 120)    \t\t(10295,)\n",
      "Test                     \t(9585, 10, 120)     \t\t(9585,)\n",
      "Stored all datasets.\n",
      "CPU times: total: 1min 50s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Remove the indices related to mouthings if we have mouthings = False\n",
    "if not mouthings:\n",
    "    for video in ling_features:\n",
    "        # Remove from the landmark feature set (for 2 landmarks, we have to remove twice as many coordinates (x, y))\n",
    "        ling_features[video] = ling_features[video][:, :-(2 * len(mouthing_ind))]\n",
    "        ling_mirrored[video] = ling_mirrored[video][:, :-(2 * len(mouthing_ind))]\n",
    "\n",
    "# Process the linguistic feature set\n",
    "features_str = '_only_ling'\n",
    "X, y, stats, var_len, ann_lengths, glosses = make_dataset(anns_with_tiers, ling_features, ling_mirrored,\n",
    "                                                          df, val_vids, test_vids, top_signs, id_split, \n",
    "                                                          zero_pad=True)\n",
    "X_train_var_len, y_train_var_len = var_len\n",
    "\n",
    "# Print an example\n",
    "print('Example from X_train:', np.array(X[0][0]))\n",
    "# Display information about the annotation length\n",
    "print_ann_length_stats(ann_lengths, stats)\n",
    "# Encode the (string) labels (to convert them to int labels)\n",
    "fitted, X, y = encode_labels(X, y, top_signs)\n",
    "# Preprocess the data (normalize, remove NaNs, etc.)\n",
    "X, y = prepare_data(X, y, fitted, normalize, features_str, mouthings_str)\n",
    "# Save the data\n",
    "save_dataset(X, y, features_str, mouthings_str, top_suffix)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8b9a8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CNGT files when split into numpy files: 4275\n",
      "Feature-extracted data exists, loading...\n",
      "Done!\n",
      "CPU times: total: 1.67 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Extract the landmark features \n",
    "lmrk_features, lmrk_mirrored = extract_features(anns_with_tiers, linguistic_features = False, mouthing_ind = mouthing_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4860df21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 855/855 [01:38<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from X_train: [[ 0.15076137  0.34367633  0.17652202  0.26578987  0.15998888  0.1808011\n",
      "   0.10913372  0.11031985 -0.0891459   0.26260012 -0.04855037  0.18104464\n",
      "  -0.24313283  0.14812303 -0.36075974  0.1185661  -0.46700025  0.09448856\n",
      "  -0.16265583  0.23631626 -0.3554976   0.16610467 -0.45491695  0.11758006\n",
      "  -0.53315115  0.08361977 -0.24396682  0.26631808 -0.34596038  0.19743133\n",
      "  -0.26894665  0.19074893 -0.19663072  0.20142204 -0.0322125   0.22253287\n",
      "  -0.33732247  0.22467726 -0.2785883   0.22154921 -0.22249818  0.2332815\n",
      "  -0.8515806   0.4952376  -0.71510553  0.4354874  -0.5632038   0.41410244\n",
      "  -0.42610765  0.41354668 -0.75906706  0.45531225 -0.6619568   0.39986658\n",
      "  -0.5032389   0.33993196 -0.41059017  0.36001456 -0.38010526  0.37977874\n",
      "  -0.7428005   0.3661238  -0.5314028   0.38043892 -0.43020272  0.40595794\n",
      "  -0.40155196  0.4259187  -0.7953844   0.40544558 -0.57252526  0.42472756\n",
      "  -0.5265188   0.44477808 -0.551321    0.45487738 -0.62910557  0.41058815\n",
      "  -0.63551474  0.46454477 -0.5965061   0.47420073 -0.61425924  0.47811294]\n",
      " [ 0.18534207  0.35392118  0.29357243  0.2947436   0.34142542  0.21651202\n",
      "   0.3551073   0.15120059 -0.02548647  0.25663412  0.01602626  0.18195575\n",
      "  -0.04572821  0.1859504  -0.15409851  0.15216196 -0.23694229  0.12594229\n",
      "  -0.01531434  0.25013125 -0.16360307  0.16446078 -0.20589256  0.10442328\n",
      "  -0.21680522  0.0663045  -0.11100364  0.26519674 -0.16540289  0.1803996\n",
      "  -0.11357045  0.15665013 -0.05801392  0.15573978  0.02774644  0.23289806\n",
      "  -0.16554856  0.20726532 -0.1221714   0.1841361  -0.08233714  0.17890227\n",
      "  -0.81848943  0.48675013 -0.70702004  0.4366281  -0.5537319   0.4149499\n",
      "  -0.4200132   0.4182533  -0.72335076  0.4514506  -0.6263294   0.39056396\n",
      "  -0.47795963  0.34397745 -0.3949833   0.363186   -0.36703753  0.38287437\n",
      "  -0.7193587   0.37186682 -0.48756313  0.3869176  -0.38498592  0.4142232\n",
      "  -0.33901143  0.43270993 -0.77024126  0.41053128 -0.523469    0.4344262\n",
      "  -0.5166998   0.45489657 -0.5669584   0.46172154 -0.59679174  0.40120995\n",
      "  -0.5961187   0.47204876 -0.60123086  0.48002923 -0.65372825  0.4794277 ]\n",
      " [ 0.24429417  0.36468685  0.39036798  0.28864568  0.44523644  0.22398192\n",
      "   0.47306204  0.16464692  0.09058285  0.27371246  0.14688253  0.19828385\n",
      "   0.05350947  0.1485802  -0.01905441  0.10166019 -0.07248569  0.06877398\n",
      "   0.07676673  0.23771483 -0.01951098  0.17247999  0.05516076  0.16826206\n",
      "   0.12538195  0.18848044  0.0101099   0.2629032  -0.01494551  0.20953763\n",
      "   0.05437469  0.2118274   0.09691501  0.22902405  0.11376643  0.24386162\n",
      "  -0.00127792  0.24624026  0.0551331   0.24978137  0.08663416  0.26371092\n",
      "  -0.79534245  0.48812366 -0.73842335  0.45061004 -0.58175826  0.4216318\n",
      "  -0.4586079   0.41634262 -0.6909926   0.46152735 -0.6024811   0.39805377\n",
      "  -0.46619678  0.3812486  -0.3938501   0.4085611  -0.38049483  0.42688584\n",
      "  -0.7185544   0.40411174 -0.48233557  0.42153192 -0.48943067  0.44339204\n",
      "  -0.55496716  0.44772136 -0.76719177  0.43895757 -0.5348115   0.46936738\n",
      "  -0.5639372   0.4789704  -0.6346183   0.4726199  -0.5829425   0.4046899\n",
      "  -0.61501503  0.5010978  -0.6363046   0.50512135 -0.69871604  0.49667597]\n",
      " [ 0.28889608  0.3728302   0.44297004  0.31261867  0.49535513  0.24282539\n",
      "   0.5399957   0.1846224   0.14147997  0.29098684  0.22226644  0.21207404\n",
      "   0.09767604  0.1756444   0.03752041  0.12909943  0.00238466  0.09546149\n",
      "   0.15383124  0.26537204  0.07804728  0.19835073  0.1440537   0.18801421\n",
      "   0.21427584  0.20483506  0.08651495  0.29146618  0.04490018  0.22941494\n",
      "   0.10278273  0.22208995  0.15692306  0.23626     0.17910504  0.2515568\n",
      "   0.02942204  0.26449412  0.07964444  0.2547853   0.12246442  0.26380712\n",
      "  -0.7597983   0.49778503         nan         nan         nan         nan\n",
      "          nan         nan -0.65353715  0.48836964 -0.57935464  0.42962927\n",
      "          nan         nan         nan         nan         nan         nan\n",
      "          nan         nan         nan         nan         nan         nan\n",
      "          nan         nan         nan         nan         nan         nan\n",
      "          nan         nan         nan         nan -0.56595266  0.42592806\n",
      "          nan         nan         nan         nan         nan         nan]\n",
      " [ 0.3336296   0.40161526  0.44249868  0.3314011   0.47924113  0.26332575\n",
      "   0.49887538  0.20439667  0.23175931  0.332752    0.28001475  0.26270396\n",
      "   0.08638716  0.2142089   0.02062941  0.17032105 -0.01794982  0.13938606\n",
      "   0.1363113   0.29991323  0.04667759  0.22710645  0.10307479  0.19896102\n",
      "   0.17212796  0.19612634  0.07936192  0.32087314  0.03298497  0.2523179\n",
      "   0.09535408  0.23263621  0.15587616  0.23589545  0.23850322  0.2943986\n",
      "   0.02805948  0.28077686  0.08709955  0.26141977  0.13976598  0.26257032\n",
      "  -0.81486094  0.54148114 -0.8170048   0.5506995  -0.6688888   0.54187596\n",
      "  -0.55726755  0.54997957 -0.66810477  0.5762991  -0.6075164   0.5127275\n",
      "  -0.5607977   0.5330559  -0.57174695  0.5559741  -0.5944185   0.57067466\n",
      "  -0.6989176   0.5260582  -0.61394083  0.55942047 -0.64071727  0.5748342\n",
      "  -0.6667197   0.5804806  -0.76689017  0.54731023 -0.702585    0.5837189\n",
      "  -0.73260856  0.58892035 -0.7621325   0.5840539  -0.6099534   0.49399114\n",
      "  -0.78652894  0.5984278  -0.80264914  0.59585905 -0.8268434   0.5849937 ]\n",
      " [ 0.3400383   0.4227456   0.30761504  0.37915778  0.23038316  0.31677806\n",
      "   0.1241045   0.27616292  0.25795674  0.36500406  0.27569366  0.29940844\n",
      "   0.3235786   0.2347756   0.3197155   0.20409447  0.32284808  0.17640382\n",
      "   0.3126359   0.3131128   0.14821792  0.27091753  0.07172799  0.27753377\n",
      "   0.04305196  0.2863294   0.23785448  0.32967073  0.08053374  0.28387952\n",
      "   0.05416775  0.300604    0.06996155  0.32098955  0.22824979  0.32961726\n",
      "   0.03368974  0.30422562  0.01314783  0.30988252  0.03511858  0.32368332\n",
      "  -0.8454901   0.57665384         nan         nan         nan         nan\n",
      "          nan         nan -0.7306217   0.6285753  -0.62149715  0.5790236\n",
      "          nan         nan         nan         nan         nan         nan\n",
      "          nan         nan         nan         nan         nan         nan\n",
      "          nan         nan         nan         nan         nan         nan\n",
      "          nan         nan         nan         nan -0.6095598   0.55463755\n",
      "          nan         nan         nan         nan         nan         nan]\n",
      " [ 0.33506918  0.46952963         nan         nan         nan         nan\n",
      "          nan         nan  0.1568296   0.45472825  0.18022871  0.37549835\n",
      "          nan         nan         nan         nan         nan         nan\n",
      "          nan         nan         nan         nan         nan         nan\n",
      "          nan         nan         nan         nan         nan         nan\n",
      "          nan         nan         nan         nan  0.18150377  0.38387138\n",
      "          nan         nan         nan         nan         nan         nan\n",
      "  -0.9363384   0.61044526         nan         nan         nan         nan\n",
      "          nan         nan -0.83046865  0.67357886 -0.74038744  0.638994\n",
      "          nan         nan         nan         nan         nan         nan\n",
      "          nan         nan         nan         nan         nan         nan\n",
      "          nan         nan         nan         nan         nan         nan\n",
      "          nan         nan         nan         nan -0.73234415  0.60272133\n",
      "          nan         nan         nan         nan         nan         nan]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10295,) (9585,)\n",
      "Unique glosses: 2712\n",
      "Mean shape: (84,) std shape: (84,)\n",
      "\tdataset \t\t\tdata shape \t\t label shape\n",
      "Train                    \t(180432, 10, 84)    \t\t(180432,)\n",
      "Train (no mirror)        \t(90216, 10, 84)     \t\t(90216,)\n",
      "Validation               \t(10295, 10, 84)     \t\t(10295,)\n",
      "Test                     \t(9585, 10, 84)      \t\t(9585,)\n",
      "Stored all datasets.\n",
      "CPU times: total: 1min 41s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Remove the indices related to mouthings if we have mouthings = False\n",
    "if not mouthings:\n",
    "    for video in lmrk_features:\n",
    "        # Remove from the landmark feature set (for 2 landmarks, we have to remove twice as many coordinates (x, y))\n",
    "        lmrk_features[video] = lmrk_features[video][:, :-(2 * len(mouthing_ind))]\n",
    "        lmrk_mirrored[video] = lmrk_mirrored[video][:, :-(2 * len(mouthing_ind))]\n",
    "\n",
    "# Process the landmark feature set\n",
    "features_str = '_only_lmrks'\n",
    "X, y, stats, var_len, ann_lengths, glosses = make_dataset(anns_with_tiers, lmrk_features, lmrk_mirrored,\n",
    "                                                          df, val_vids, test_vids, top_signs, id_split, \n",
    "                                                          zero_pad=True)\n",
    "X_train_var_len, y_train_var_len = var_len\n",
    "\n",
    "# Do the same steps as for the linguistic features\n",
    "print('Example from X_train:', np.array(X[0][0]))\n",
    "# print_ann_length_stats(ann_lengths, stats)\n",
    "fitted, X, y = encode_labels(X, y, top_signs)\n",
    "X, y = prepare_data(X, y, fitted, normalize, features_str, mouthings_str)\n",
    "save_dataset(X, y, features_str, mouthings_str, top_suffix)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "131394c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 855/855 [08:50<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of video data shape with combined features: (3524, 204)\n",
      "CPU times: total: 26.3 s\n",
      "Wall time: 17min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The paths for the combined features\n",
    "features_str = '_combined'\n",
    "features_path = PATHS['features_data'].format(features_str)\n",
    "mirrored_path = PATHS['mirrored_features_data'].format(features_str)\n",
    "\n",
    "# Concatenate the data of the linguistic and landmark features\n",
    "combined_features = {}\n",
    "combined_mirrored = {}\n",
    "\n",
    "for video in tqdm(ling_features):\n",
    "    combined_features[video] = np.append(ling_features[video], lmrk_features[video], axis = 1)\n",
    "    combined_mirrored[video] = np.append(ling_mirrored[video], lmrk_mirrored[video], axis = 1)\n",
    "    \n",
    "print('Example of video data shape with combined features:', combined_features[video].shape)\n",
    "\n",
    "# Save the combined features set\n",
    "features_data_np = np.array(list(combined_features.items()), dtype=object)\n",
    "np.save(features_path, features_data_np)\n",
    "\n",
    "# Same for the mirrored data\n",
    "mirrored_data_np = np.array(list(combined_mirrored.items()), dtype=object)\n",
    "np.save(mirrored_path, mirrored_data_np) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8c34215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 855/855 [02:16<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from X_train: [[ 0.46017638  0.40214708  0.9784996  ...  0.47420073 -0.61425924\n",
      "   0.47811294]\n",
      " [ 0.29058886  0.19711134  0.8826425  ...  0.48002923 -0.65372825\n",
      "   0.4794277 ]\n",
      " [ 0.23168114  0.16862081  0.9759804  ...  0.50512135 -0.69871604\n",
      "   0.49667597]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "(10295,) (9585,)\n",
      "Unique glosses: 2712\n",
      "Mean shape: (204,) std shape: (204,)\n",
      "\tdataset \t\t\tdata shape \t\t label shape\n",
      "Train                    \t(180432, 10, 204)   \t\t(180432,)\n",
      "Train (no mirror)        \t(90216, 10, 204)    \t\t(90216,)\n",
      "Validation               \t(10295, 10, 204)    \t\t(10295,)\n",
      "Test                     \t(9585, 10, 204)     \t\t(9585,)\n",
      "Stored all datasets.\n",
      "CPU times: total: 2min\n",
      "Wall time: 5min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Process the combined (linguistic + landmark) feature set\n",
    "X, y, stats, var_len, ann_lengths, glosses = make_dataset(anns_with_tiers, combined_features, combined_mirrored,\n",
    "                                                          df, val_vids, test_vids, top_signs, id_split, \n",
    "                                                          zero_pad=True)\n",
    "X_train_var_len, y_train_var_len = var_len\n",
    "\n",
    "# Do the same steps as for the linguistic features\n",
    "print('Example from X_train:', np.array(X[0][0]))\n",
    "# print_ann_length_stats(ann_lengths, stats)\n",
    "fitted, X, y = encode_labels(X, y, top_signs)\n",
    "X, y = prepare_data(X, y, fitted, normalize, features_str, mouthings_str)\n",
    "save_dataset(X, y, features_str, mouthings_str, top_suffix)\n",
    "# del X, y\n",
    "del combined_features, combined_mirrored\n",
    "del lmrk_features, lmrk_mirrored\n",
    "del ling_features, ling_mirrored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ed504aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(786, 178, 1171, 778, 1565)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = load_dict(PATHS['label_encoder'].format(top_suffix))\n",
    "labels['GEBOREN-A'], labels['AMSTERDAM'], labels['KLAAR-A'], labels['GEBAREN-A'], labels['NU-A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad8090a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(X):\n",
    "    shape = X.shape\n",
    "    X_ = X.reshape(-1, shape[-1])\n",
    "    mean, std = np.mean(X_, axis = 0), np.std(X_, axis = 0)\n",
    "    print('Mean:', np.round(mean, 3))\n",
    "    print('Std:', np.round(std, 3))\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbc860da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files (x86)\\python39\\lib\\site-packages\\numpy\\core\\_methods.py:236: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "d:\\program files (x86)\\python39\\lib\\site-packages\\numpy\\core\\_methods.py:247: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [ 0.06   0.005 -0.017 -0.001 -0.034 -0.032 -0.046 -0.046 -0.039  0.02\n",
      "  0.038 -0.017  0.001 -0.028 -0.009 -0.039 -0.01  -0.034 -0.006 -0.063\n",
      " -0.024 -0.005  0.051 -0.008  0.016 -0.003 -0.053  0.121  0.137 -0.005\n",
      " -0.023 -0.023 -0.023 -0.02  -0.043 -0.047 -0.046 -0.035  0.    -0.015\n",
      " -0.027 -0.021 -0.035 -0.041  0.007 -0.232 -0.013 -0.298 -0.008 -0.226\n",
      " -0.017 -0.216 -0.021 -0.21  -0.022 -0.221 -0.001  0.    -0.009 -0.054\n",
      " -0.005  0.017  0.001  0.035  0.034  0.047  0.048  0.039 -0.02  -0.034\n",
      "  0.017 -0.004  0.029  0.007  0.04   0.009  0.033  0.004  0.082  0.027\n",
      "  0.005 -0.046  0.008 -0.016  0.003  0.056 -0.105 -0.112  0.006  0.032\n",
      "  0.029  0.029  0.025  0.071  0.07   0.066  0.049 -0.     0.025  0.046\n",
      "  0.032  0.059  0.068  0.013  0.287 -0.011  0.329 -0.016  0.222 -0.013\n",
      "  0.223 -0.012  0.228 -0.011  0.239  0.002 -0.001  0.007  0.     0.\n",
      "  0.007 -0.232  0.005 -0.213  0.004 -0.226  0.003 -0.236  0.003 -0.259\n",
      "  0.004 -0.264  0.002 -0.247  0.001 -0.251  0.001 -0.247  0.003 -0.235\n",
      "  0.002 -0.242  0.002 -0.241  0.002 -0.239  0.002 -0.231  0.002 -0.234\n",
      "  0.002 -0.232  0.002 -0.23   0.005 -0.257  0.002 -0.227  0.002 -0.226\n",
      "  0.002 -0.226  0.013  0.287  0.019  0.292  0.017  0.314  0.014  0.331\n",
      "  0.009  0.327  0.009  0.338  0.01   0.346  0.007  0.345  0.006  0.345\n",
      "  0.012  0.328  0.007  0.337  0.007  0.335  0.007  0.332  0.011  0.318\n",
      "  0.006  0.328  0.006  0.321  0.007  0.32   0.011  0.329  0.006  0.317\n",
      "  0.006  0.316  0.007  0.316]\n",
      "Std: [0.135 0.095 0.048 0.048 0.048 0.048 0.048 0.048 0.067 0.067 0.135 0.048\n",
      " 0.048 0.048 0.048 0.048 0.048 0.067 0.048   inf   inf   inf   inf 0.135\n",
      " 0.095 0.067 0.067 0.135 0.135   inf   inf   inf   inf   inf   inf   inf\n",
      "   inf   inf   inf   inf   inf   inf   inf   inf   inf 0.135   inf 0.131\n",
      "   inf 0.137   inf 0.143   inf 0.152   inf 0.157   inf   inf   inf 0.095\n",
      " 0.095 0.067 0.048 0.048 0.067 0.067 0.067 0.067 0.067 0.135 0.048 0.048\n",
      " 0.048 0.048 0.048 0.048 0.067 0.048   inf   inf   inf   inf 0.135 0.095\n",
      " 0.067 0.067 0.095 0.095   inf   inf   inf   inf   inf   inf   inf   inf\n",
      "   inf   inf   inf   inf   inf   inf   inf   inf 0.152   inf 0.152   inf\n",
      " 0.138   inf 0.146   inf 0.166   inf 0.172   inf   inf   inf   inf   inf\n",
      "   inf 0.135   inf 0.124   inf 0.132   inf 0.135   inf 0.135   inf 0.138\n",
      "   inf 0.135   inf 0.135   inf 0.135   inf 0.133   inf 0.13    inf 0.132\n",
      "   inf 0.135   inf 0.127   inf 0.124   inf 0.125   inf 0.129   inf 0.138\n",
      "   inf 0.12    inf 0.12    inf 0.121   inf 0.152   inf   inf   inf   inf\n",
      "   inf   inf   inf 0.154   inf 0.171   inf   inf   inf   inf   inf   inf\n",
      "   inf   inf   inf   inf   inf   inf   inf   inf   inf   inf   inf   inf\n",
      "   inf   inf   inf   inf   inf 0.17    inf   inf   inf   inf   inf   inf]\n",
      "X-train (no mirroring)\n",
      "Mean: [-0.     0.    -0.    -0.    -0.    -0.    -0.    -0.    -0.    -0.\n",
      " -0.    -0.    -0.001 -0.001 -0.001 -0.    -0.001 -0.    -0.001 -0.\n",
      " -0.    -0.     0.    -0.     0.    -0.     0.     0.     0.    -0.\n",
      "  0.    -0.    -0.     0.    -0.    -0.    -0.    -0.     0.    -0.\n",
      " -0.    -0.     0.     0.     0.    -0.    -0.     0.    -0.    -0.\n",
      "  0.    -0.     0.     0.     0.    -0.    -0.    -0.     0.    -0.\n",
      " -0.    -0.    -0.    -0.    -0.    -0.     0.    -0.    -0.    -0.\n",
      " -0.    -0.001 -0.    -0.001 -0.    -0.001 -0.    -0.001  0.     0.\n",
      "  0.    -0.    -0.     0.     0.    -0.     0.     0.     0.     0.\n",
      "  0.    -0.     0.    -0.     0.     0.     0.     0.    -0.    -0.\n",
      "  0.    -0.     0.    -0.    -0.     0.     0.    -0.    -0.     0.\n",
      " -0.    -0.     0.     0.     0.    -0.    -0.     0.    -0.     0.\n",
      "  0.    -0.    -0.     0.     0.     0.     0.    -0.     0.    -0.\n",
      " -0.    -0.     0.    -0.     0.     0.     0.    -0.    -0.    -0.\n",
      "  0.     0.    -0.    -0.    -0.     0.     0.    -0.     0.    -0.\n",
      " -0.     0.    -0.    -0.    -0.    -0.     0.    -0.    -0.    -0.\n",
      "  0.    -0.    -0.    -0.    -0.    -0.     0.     0.    -0.     0.\n",
      "  0.    -0.     0.     0.    -0.     0.    -0.    -0.     0.     0.\n",
      "  0.     0.    -0.    -0.     0.     0.     0.    -0.    -0.    -0.\n",
      " -0.     0.    -0.     0.    -0.    -0.     0.    -0.    -0.    -0.\n",
      "  0.    -0.     0.     0.   ]\n",
      "Std: [0.19  0.135 0.067 0.067 0.067 0.067 0.067 0.067 0.095 0.095 0.19  0.067\n",
      " 0.067 0.067 0.067 0.067 0.067 0.095 0.067   inf   inf   inf   inf 0.19\n",
      " 0.135 0.095 0.095 0.19  0.19    inf   inf   inf   inf   inf   inf   inf\n",
      "   inf   inf   inf   inf   inf   inf   inf   inf   inf 0.156   inf 0.174\n",
      "   inf 0.192   inf 0.196   inf 0.203   inf 0.205   inf   inf   inf 0.135\n",
      " 0.135 0.095 0.067 0.067 0.095 0.095 0.095 0.095 0.095 0.19  0.067 0.067\n",
      " 0.067 0.067 0.067 0.067 0.095 0.067   inf   inf   inf   inf 0.19  0.135\n",
      " 0.095 0.095 0.135 0.135   inf   inf   inf   inf   inf   inf   inf   inf\n",
      "   inf   inf   inf   inf   inf   inf   inf   inf 0.209   inf 0.193   inf\n",
      " 0.192   inf 0.197   inf 0.21    inf 0.216   inf   inf   inf   inf 0.26\n",
      "   inf 0.156   inf 0.145   inf 0.153   inf 0.162   inf 0.156   inf 0.162\n",
      "   inf 0.16    inf 0.158   inf 0.16    inf 0.153   inf 0.149   inf 0.152\n",
      "   inf 0.156   inf 0.149   inf 0.146   inf 0.146   inf 0.15    inf 0.162\n",
      "   inf 0.144   inf 0.144   inf 0.144   inf 0.209   inf 0.23    inf   inf\n",
      "   inf   inf   inf 0.208   inf 0.217   inf   inf   inf   inf   inf   inf\n",
      "   inf   inf   inf   inf   inf   inf   inf   inf   inf   inf   inf   inf\n",
      "   inf   inf   inf   inf   inf 0.219   inf 0.268   inf 0.261   inf 0.258]\n",
      "X-val\n",
      "Mean: [ 0.039  0.004  0.007 -0.015 -0.006  0.01   0.007  0.008  0.034 -0.001\n",
      "  0.032  0.002 -0.037 -0.014 -0.031 -0.004 -0.027  0.028 -0.024 -0.042\n",
      " -0.062 -0.044 -0.068 -0.049 -0.057  0.023 -0.005  0.044 -0.024 -0.005\n",
      " -0.022 -0.015 -0.006 -0.009 -0.032 -0.033 -0.028 -0.035 -0.047 -0.044\n",
      " -0.042 -0.038 -0.036 -0.029  0.026 -0.05   0.016 -0.054  0.03  -0.043\n",
      "  0.075 -0.053  0.083 -0.049  0.057 -0.039  0.025  0.001 -0.003 -0.02\n",
      "  0.008 -0.004  0.009  0.048  0.006  0.057  0.009  0.003 -0.043 -0.017\n",
      " -0.006  0.024  0.05   0.02   0.058  0.021  0.005  0.019  0.064  0.079\n",
      "  0.068  0.079  0.016  0.024  0.014 -0.072  0.1    0.14   0.016  0.028\n",
      "  0.017  0.01  -0.001  0.033  0.02   0.016  0.007  0.047  0.051  0.055\n",
      "  0.035  0.048  0.05   0.002 -0.114  0.016 -0.125  0.055 -0.046  0.05\n",
      " -0.045  0.039 -0.049  0.024 -0.042 -0.001  0.    -0.     0.017 -0.049\n",
      "  0.026 -0.05   0.023 -0.09   0.024 -0.091  0.024 -0.092  0.025 -0.053\n",
      "  0.026 -0.061  0.029 -0.099  0.029 -0.097  0.029 -0.095  0.029 -0.098\n",
      "  0.034 -0.1    0.037 -0.099  0.039 -0.096  0.03  -0.097  0.034 -0.098\n",
      "  0.037 -0.096  0.04  -0.093  0.027 -0.06   0.032 -0.094  0.034 -0.092\n",
      "  0.036 -0.089  0.002 -0.114  0.003 -0.086  0.012 -0.092  0.019 -0.098\n",
      "  0.016 -0.129  0.014 -0.131  0.031 -0.1    0.039 -0.1    0.046 -0.1\n",
      "  0.018 -0.098  0.031 -0.101  0.04  -0.099  0.05  -0.096  0.015 -0.098\n",
      "  0.025 -0.098  0.033 -0.096  0.041 -0.095  0.011 -0.125  0.015 -0.094\n",
      "  0.02  -0.092  0.026 -0.09 ]\n",
      "Std: [0.52  0.399 0.2   0.2   0.2   0.2   0.2   0.2   0.282 0.282 0.564 0.2\n",
      " 0.2   0.2   0.2   0.2   0.2   0.282 0.2   0.674 0.642 0.74  0.754 0.45\n",
      " 0.399 0.282 0.282 0.564 0.564 0.568 0.613 0.646 0.634 0.612 0.626 0.654\n",
      " 0.653 0.626 0.568 0.571 0.578 0.636 0.619 0.618   inf 0.368   inf 0.402\n",
      "   inf 0.436   inf 0.487   inf 0.554   inf 0.558   inf   inf   inf 0.399\n",
      " 0.399 0.282 0.2   0.2   0.282 0.282 0.282 0.282 0.282 0.564 0.2   0.2\n",
      " 0.2   0.2   0.2   0.2   0.282 0.2   0.695 0.696   inf 0.684 0.488 0.399\n",
      " 0.282 0.282 0.399 0.399 0.496 0.442 0.457 0.479 0.489 0.486 0.485 0.538\n",
      " 0.58  0.6   0.572 0.566 0.566 0.574 0.613 0.498 0.469   inf 0.423   inf\n",
      " 0.423   inf 0.486   inf 0.564   inf 0.564 0.383 0.383 0.6     inf 0.508\n",
      "   inf 0.368   inf 0.406   inf 0.411   inf 0.414   inf 0.399   inf 0.402\n",
      "   inf 0.413   inf 0.416   inf 0.417   inf 0.411   inf 0.412   inf 0.414\n",
      "   inf 0.416   inf 0.41    inf 0.41    inf 0.412   inf 0.413   inf 0.401\n",
      "   inf 0.408   inf 0.409   inf 0.41  0.498 0.469   inf 0.564   inf 0.57\n",
      "   inf 0.572   inf 0.538   inf 0.564   inf 0.57    inf 0.57    inf 0.57\n",
      "   inf 0.566   inf 0.568   inf 0.568   inf 0.568   inf 0.566   inf 0.568\n",
      "   inf 0.568   inf 0.568   inf 0.548   inf 0.566   inf 0.566   inf 0.566]\n",
      "X-test\n",
      "Mean: [-0.12  -0.131 -0.087 -0.105 -0.087 -0.094 -0.076 -0.105 -0.113 -0.148\n",
      " -0.098 -0.093 -0.118 -0.096 -0.114 -0.086 -0.112 -0.122 -0.112  0.014\n",
      " -0.007  0.003 -0.045 -0.091 -0.039 -0.063 -0.129  0.016 -0.066 -0.051\n",
      " -0.052 -0.062 -0.06  -0.059 -0.014 -0.03  -0.04  -0.05  -0.031 -0.015\n",
      " -0.005  0.001  0.007  0.006  0.004  0.049  0.001  0.131  0.     0.055\n",
      "  0.005  0.054  0.007  0.053  0.01   0.05  -0.003  0.001  0.009 -0.046\n",
      "  0.013 -0.047 -0.044 -0.05  -0.006 -0.054  0.002 -0.035 -0.058 -0.064\n",
      " -0.048 -0.061 -0.051 -0.062 -0.058 -0.056 -0.036 -0.059 -0.014 -0.024\n",
      " -0.    -0.022 -0.032 -0.024 -0.051 -0.092 -0.024 -0.063 -0.028 -0.031\n",
      " -0.029 -0.023 -0.023 -0.022 -0.029 -0.024 -0.023 -0.02  -0.012 -0.01\n",
      " -0.017 -0.017 -0.019  0.003  0.06   0.002  0.079 -0.003  0.101 -0.003\n",
      "  0.104 -0.003  0.111 -0.002  0.127 -0.003  0.001  0.01   0.002  0.051\n",
      "  0.004  0.049  0.001 -0.068  0.001 -0.059  0.001 -0.054  0.004  0.067\n",
      "  0.003  0.071  0.001 -0.054  0.001 -0.053  0.001 -0.052  0.001 -0.061\n",
      "  0.001 -0.058  0.002 -0.055  0.002 -0.053  0.001 -0.064  0.002 -0.061\n",
      "  0.002 -0.057  0.002 -0.055  0.003  0.066  0.002 -0.063  0.002 -0.06\n",
      "  0.003 -0.057  0.003  0.06   0.007  0.066  0.007  0.078  0.006  0.086\n",
      "  0.006  0.07   0.006  0.064  0.006  0.097  0.006  0.096  0.006  0.095\n",
      "  0.008  0.091  0.006  0.095  0.006  0.093  0.006  0.09   0.008  0.088\n",
      "  0.006  0.093  0.005  0.09   0.006  0.087  0.005  0.062  0.006  0.09\n",
      "  0.005  0.089  0.006  0.088]\n",
      "Std: [0.531 0.413 0.207 0.207 0.207 0.207 0.207 0.217 0.292 0.292 0.584 0.207\n",
      " 0.207 0.207 0.207 0.207 0.207 0.292 0.207 0.78  0.755 0.826 0.802 0.457\n",
      " 0.413 0.292 0.292 0.584 0.584   inf   inf   inf   inf   inf   inf   inf\n",
      "   inf 0.788 0.742   inf   inf   inf   inf   inf 0.116 0.409 0.111 0.426\n",
      " 0.096 0.446 0.237 0.472 0.282 0.514 0.271 0.518 0.045 0.045 0.798 0.413\n",
      " 0.413 0.207 0.207 0.207 0.292 0.292 0.292 0.292 0.292 0.574 0.207 0.207\n",
      " 0.207 0.207 0.207 0.207 0.292 0.207 0.726 0.722 0.784 0.712 0.474 0.413\n",
      " 0.292 0.292 0.413 0.413 0.686 0.702 0.686 0.678 0.669 0.809 0.746 0.715\n",
      " 0.688 0.719 0.722 0.728 0.688 0.727 0.723 0.214 0.53  0.13  0.454 0.207\n",
      " 0.437 0.199 0.486 0.191 0.564 0.183 0.588 0.119 0.12  0.616 0.139 0.526\n",
      " 0.116 0.409 0.084 0.418 0.084 0.423 0.084 0.432 0.117 0.42  0.101 0.428\n",
      " 0.106 0.43  0.107 0.425 0.109 0.425 0.111 0.422 0.128 0.424 0.14  0.422\n",
      " 0.149 0.425 0.121 0.421 0.137 0.422 0.146 0.423 0.156 0.425 0.101 0.43\n",
      " 0.138 0.42  0.147 0.42  0.156 0.422 0.214 0.53  0.271 0.591 0.29  0.595\n",
      " 0.298 0.626 0.292 0.59  0.226 0.596 0.337 0.608 0.336 0.622 0.334 0.622\n",
      " 0.322 0.602 0.328 0.608 0.33  0.62  0.339 0.622 0.328 0.599 0.317 0.609\n",
      " 0.321 0.618 0.338 0.618 0.201 0.594 0.301 0.602 0.308 0.614 0.326 0.614]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((786, 178, 1171, 778, 1565),\n",
       " (array([-1.1969e-01, -1.3110e-01, -8.6975e-02, -1.0516e-01, -8.7463e-02,\n",
       "         -9.4299e-02, -7.6172e-02, -1.0492e-01, -1.1279e-01, -1.4746e-01,\n",
       "         -9.7900e-02, -9.3262e-02, -1.1798e-01, -9.5642e-02, -1.1432e-01,\n",
       "         -8.6060e-02, -1.1249e-01, -1.2250e-01, -1.1237e-01,  1.3786e-02,\n",
       "         -7.0000e-03,  3.1147e-03, -4.5074e-02, -9.0820e-02, -3.8788e-02,\n",
       "         -6.2744e-02, -1.2939e-01,  1.6129e-02, -6.6345e-02, -5.1361e-02,\n",
       "         -5.2246e-02, -6.2042e-02, -5.9784e-02, -5.8533e-02, -1.4137e-02,\n",
       "         -2.9938e-02, -4.0344e-02, -4.9835e-02, -3.0838e-02, -1.5213e-02,\n",
       "         -5.2452e-03,  1.3266e-03,  6.5384e-03,  5.6419e-03,  4.4670e-03,\n",
       "          4.9133e-02,  9.1839e-04,  1.3062e-01,  1.4007e-04,  5.5267e-02,\n",
       "          4.6501e-03,  5.4352e-02,  7.1030e-03,  5.3131e-02,  1.0323e-02,\n",
       "          5.0110e-02, -3.2272e-03,  1.3428e-03,  9.3613e-03, -4.5868e-02,\n",
       "          1.3382e-02, -4.6844e-02, -4.4342e-02, -4.9561e-02, -5.6381e-03,\n",
       "         -5.4474e-02,  2.1248e-03, -3.5431e-02, -5.8075e-02, -6.4148e-02,\n",
       "         -4.7760e-02, -6.0822e-02, -5.1178e-02, -6.2225e-02, -5.7770e-02,\n",
       "         -5.6335e-02, -3.5736e-02, -5.8838e-02, -1.4015e-02, -2.4155e-02,\n",
       "         -2.0325e-04, -2.1881e-02, -3.2410e-02, -2.4307e-02, -5.1392e-02,\n",
       "         -9.2224e-02, -2.3987e-02, -6.3416e-02, -2.8015e-02, -3.1464e-02,\n",
       "         -2.8885e-02, -2.2949e-02, -2.3224e-02, -2.1957e-02, -2.9297e-02,\n",
       "         -2.3956e-02, -2.2995e-02, -1.9653e-02, -1.2451e-02, -1.0078e-02,\n",
       "         -1.7441e-02, -1.6617e-02, -1.8997e-02,  3.2291e-03,  6.0120e-02,\n",
       "          2.1591e-03,  7.8613e-02, -2.8934e-03,  1.0132e-01, -2.5349e-03,\n",
       "          1.0358e-01, -2.5215e-03,  1.1072e-01, -2.3594e-03,  1.2732e-01,\n",
       "         -3.0727e-03,  6.0892e-04,  9.7961e-03,  1.9331e-03,  5.0812e-02,\n",
       "          4.4670e-03,  4.9133e-02,  7.8106e-04, -6.7749e-02,  6.6471e-04,\n",
       "         -5.8594e-02,  6.4135e-04, -5.4016e-02,  3.9101e-03,  6.7078e-02,\n",
       "          2.8076e-03,  7.0984e-02,  6.5279e-04, -5.3833e-02,  6.5660e-04,\n",
       "         -5.2795e-02,  6.7425e-04, -5.1697e-02,  1.0576e-03, -6.1035e-02,\n",
       "          1.3885e-03, -5.8136e-02,  1.5650e-03, -5.5481e-02,  1.6861e-03,\n",
       "         -5.2979e-02,  1.4658e-03, -6.4331e-02,  1.7920e-03, -6.0608e-02,\n",
       "          1.9054e-03, -5.7343e-02,  1.9932e-03, -5.4718e-02,  2.9964e-03,\n",
       "          6.6101e-02,  2.1553e-03, -6.2805e-02,  2.3460e-03, -5.9784e-02,\n",
       "          2.5330e-03, -5.7129e-02,  3.2291e-03,  6.0120e-02,  7.3051e-03,\n",
       "          6.5918e-02,  6.6376e-03,  7.7637e-02,  6.0997e-03,  8.5754e-02,\n",
       "          6.3362e-03,  7.0374e-02,  5.8937e-03,  6.4026e-02,  6.3744e-03,\n",
       "          9.6741e-02,  5.7869e-03,  9.6191e-02,  5.5962e-03,  9.5215e-02,\n",
       "          7.7477e-03,  9.1003e-02,  5.9776e-03,  9.5215e-02,  5.5428e-03,\n",
       "          9.3262e-02,  5.6343e-03,  9.0027e-02,  7.7782e-03,  8.8318e-02,\n",
       "          5.6343e-03,  9.3445e-02,  5.2643e-03,  9.0515e-02,  5.5122e-03,\n",
       "          8.7036e-02,  4.9477e-03,  6.2317e-02,  5.5618e-03,  9.0332e-02,\n",
       "          5.3482e-03,  8.9294e-02,  5.5008e-03,  8.7585e-02], dtype=float16),\n",
       "  array([0.5312 , 0.4133 , 0.2067 , 0.2067 , 0.2067 , 0.2067 , 0.2067 ,\n",
       "         0.217  , 0.2922 , 0.2922 , 0.5845 , 0.2067 , 0.2067 , 0.2067 ,\n",
       "         0.2067 , 0.2067 , 0.2067 , 0.2922 , 0.2067 , 0.7793 , 0.755  ,\n",
       "         0.8267 , 0.8013 , 0.457  , 0.4133 , 0.2922 , 0.2922 , 0.5845 ,\n",
       "         0.5845 ,     inf,     inf,     inf,     inf,     inf,     inf,\n",
       "             inf,     inf, 0.7886 , 0.7417 ,     inf,     inf,     inf,\n",
       "             inf,     inf, 0.11584, 0.4092 , 0.11115, 0.4265 , 0.09564,\n",
       "         0.446  , 0.2367 , 0.4717 , 0.2817 , 0.514  , 0.271  , 0.518  ,\n",
       "         0.04517, 0.04526, 0.7983 , 0.4133 , 0.4133 , 0.2067 , 0.2067 ,\n",
       "         0.2067 , 0.2922 , 0.2922 , 0.2922 , 0.2922 , 0.2922 , 0.574  ,\n",
       "         0.2067 , 0.2067 , 0.2067 , 0.2067 , 0.2067 , 0.2067 , 0.2922 ,\n",
       "         0.2067 , 0.7256 , 0.7227 , 0.7847 , 0.7114 , 0.4744 , 0.4133 ,\n",
       "         0.2922 , 0.2922 , 0.4133 , 0.4133 , 0.6855 , 0.7017 , 0.6855 ,\n",
       "         0.678  , 0.669  , 0.809  , 0.7456 , 0.715  , 0.6885 , 0.7188 ,\n",
       "         0.7227 , 0.7285 , 0.6875 , 0.727  , 0.723  , 0.2137 , 0.53   ,\n",
       "         0.1299 , 0.4546 , 0.2067 , 0.4373 , 0.1986 , 0.4863 , 0.1912 ,\n",
       "         0.564  , 0.1829 , 0.5884 , 0.1193 , 0.1195 , 0.6157 , 0.139  ,\n",
       "         0.5254 , 0.11584, 0.4092 , 0.08435, 0.4177 , 0.08386, 0.423  ,\n",
       "         0.08405, 0.4316 , 0.1171 , 0.4197 , 0.1008 , 0.4277 , 0.1057 ,\n",
       "         0.43   , 0.10706, 0.4253 , 0.10876, 0.4253 , 0.1114 , 0.4224 ,\n",
       "         0.1285 , 0.424  , 0.1395 , 0.4226 , 0.1494 , 0.4253 , 0.1212 ,\n",
       "         0.421  , 0.1366 , 0.422  , 0.1458 , 0.423  , 0.1555 , 0.4253 ,\n",
       "         0.1006 , 0.4294 , 0.1384 , 0.42   , 0.1471 , 0.4204 , 0.1562 ,\n",
       "         0.4214 , 0.2137 , 0.53   , 0.2712 , 0.591  , 0.2898 , 0.595  ,\n",
       "         0.2976 , 0.6255 , 0.2917 , 0.59   , 0.2261 , 0.596  , 0.3367 ,\n",
       "         0.6084 , 0.3354 , 0.622  , 0.3337 , 0.622  , 0.3215 , 0.602  ,\n",
       "         0.3286 , 0.608  , 0.3296 , 0.6206 , 0.339  , 0.6216 , 0.3281 ,\n",
       "         0.599  , 0.3167 , 0.609  , 0.3208 , 0.6177 , 0.3374 , 0.6187 ,\n",
       "         0.2013 , 0.5938 , 0.301  , 0.602  , 0.308  , 0.614  , 0.3257 ,\n",
       "         0.6147 ], dtype=float16)))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_train_no_mirr, X_val, X_test = X\n",
    "print('X-train')\n",
    "m, s = stats(X_train)\n",
    "print('X-train (no mirroring)')\n",
    "_, stats(X_train_no_mirr)\n",
    "print('X-val')\n",
    "_, stats(X_val)\n",
    "print('X-test')\n",
    "_, stats(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc4644be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 0.1212\n",
      "28 0.1366\n",
      "45 -0.2318\n",
      "47 -0.298\n",
      "49 -0.2262\n",
      "51 -0.2161\n",
      "53 -0.2101\n",
      "55 -0.2213\n",
      "86 -0.1051\n",
      "87 -0.1119\n",
      "104 0.2869\n",
      "106 0.329\n",
      "108 0.2223\n",
      "110 0.2233\n",
      "112 0.2278\n",
      "114 0.2386\n",
      "121 -0.2318\n",
      "123 -0.2128\n",
      "125 -0.2263\n",
      "127 -0.2358\n",
      "129 -0.2588\n",
      "131 -0.2637\n",
      "133 -0.247\n",
      "135 -0.251\n",
      "137 -0.2474\n",
      "139 -0.235\n",
      "141 -0.2417\n",
      "143 -0.2408\n",
      "145 -0.2393\n",
      "147 -0.2312\n",
      "149 -0.2345\n",
      "151 -0.2324\n",
      "153 -0.23\n",
      "155 -0.257\n",
      "157 -0.2272\n",
      "159 -0.2264\n",
      "161 -0.2257\n",
      "163 0.2869\n",
      "165 0.2922\n",
      "167 0.3137\n",
      "169 0.331\n",
      "171 0.3267\n",
      "173 0.3386\n",
      "175 0.3464\n",
      "177 0.345\n",
      "179 0.3452\n",
      "181 0.3284\n",
      "183 0.337\n",
      "185 0.3352\n",
      "187 0.3325\n",
      "189 0.3174\n",
      "191 0.3286\n",
      "193 0.321\n",
      "195 0.32\n",
      "197 0.3293\n",
      "199 0.317\n",
      "201 0.3162\n",
      "203 0.3154\n"
     ]
    }
   ],
   "source": [
    "for i in range(m.shape[0]):\n",
    "    if abs(m[i]) > 0.1:\n",
    "        print(i, m[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f33524",
   "metadata": {},
   "source": [
    "# Finding highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Convert the variable length train data to numpy\n",
    "X_train = np.array(X_train_var_len)\n",
    "y_train = np.array(y_train_var_len)\n",
    "\n",
    "# The above code gives us unevenly sized annotations, we can't just reshape\n",
    "# So to add each frame as a row, we do it in a loop\n",
    "x = []\n",
    "for data_point in X_train:\n",
    "    for frame in data_point:\n",
    "        x.append(frame)\n",
    "        \n",
    "# We mask any NaN values so we can ignore them during later computations\n",
    "x = np.array(x)\n",
    "x = ma.masked_invalid(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f863a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# NOTE: this takes very long to compute (45-60 minutes on the tested hardware)\n",
    "# This is why we try to avoid recomputing this if possible\n",
    "\n",
    "# We compute the correlations between the values in the train data\n",
    "corrs_path = PATHS['masked_corrs'].format(features_str)\n",
    "cols = range(x.shape[-1]) # Number of columns = last dimension of x\n",
    "\n",
    "# If we already computed the correlations, reuse them\n",
    "if os.path.exists(corrs_path):\n",
    "    print('Corrs being loaded...')\n",
    "    corrs = np.load(corrs_path)\n",
    "    # We need to convert to a masked array (can't save it like that)\n",
    "    if 'masked' in corrs_path:\n",
    "        corrs = ma.masked_invalid(corrs)\n",
    "    corrs = list(corrs)\n",
    "else: # Else, compute the correlations (this takes a while)\n",
    "    print('Corrs being created...')    \n",
    "    print('Data shape:', x.shape)\n",
    "\n",
    "    corrs = []\n",
    "    for col_1 in range(x.shape[-1]-1):\n",
    "        for col_2 in range(col_1+1, x.shape[-1]):\n",
    "            print(col_1, end = '\\r')\n",
    "            corr = ma.corrcoef(x[:,col_1], x[:, col_2])[0][1]\n",
    "            corrs.append((corr, col_1, col_2))\n",
    "    corrs_np = np.array(corrs)\n",
    "    np.save(corrs_path, corrs_np)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd196dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuation of the block above this one\n",
    "print('Total correlations:\\t\\t{}'.format(len(corrs)))\n",
    "num_high_corr = 0\n",
    "high_corrs = []\n",
    "removable = []\n",
    "corr_thresh = 0.9\n",
    "for corr in corrs:\n",
    "    if abs(corr[0]) >= corr_thresh:\n",
    "        corr = [corr[0], int(corr[1]), int(corr[2])]\n",
    "        high_corrs.append(tuple(corr))\n",
    "        # Remove the first feature of the high corr. pair\n",
    "        removable.append(sorted(corr[1:])[-1])\n",
    "        num_high_corr += 1\n",
    "print('Num. of correlations >= {}:\\t{}'.format(corr_thresh, num_high_corr))\n",
    "to_remove = list(set(removable))\n",
    "\n",
    "print('\\nTo be removed:', to_remove)\n",
    "remain = set(cols) - set(to_remove)\n",
    "\n",
    "print('\\nNumber of features we remove:', len(to_remove))\n",
    "print('Number of features remaining:', len(list(remain)))\n",
    "\n",
    "print('\\n\\nHigh correlation tuples:\\n')\n",
    "print(sorted(high_corrs, key = lambda x: x[0], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c4a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which features remain\n",
    "def feature_selection_analysis(num_features, to_remove):\n",
    "    half_features = round(num_features/2) - 1\n",
    "    print(half_features)\n",
    "    rem = set(range(x.shape[-1])) - set(to_remove)\n",
    "    print('Remain length:', len(rem))\n",
    "    rem_first = [r for r in sorted(rem) if r <= half_features]\n",
    "    rem_second = [r-(half_features+1) for r in sorted(rem) if r > half_features]\n",
    "    print('Remaining of left hand:')\n",
    "    print(rem_first, '({} features)'.format(len(rem_first)))\n",
    "    print('Remaining of right hand:')\n",
    "    print(rem_second, '({} features)'.format(len(rem_second)))\n",
    "\n",
    "    print('Extra features right hand:', sorted(set(rem_second) - set(rem_first)))\n",
    "    print('Extra features left hand:', sorted(set(rem_first) - set(rem_second)))\n",
    "    \n",
    "    print('Same for both hands:', sorted(set(rem_second) & set(rem_first)))\n",
    "# We use cols to get the number of features\n",
    "feature_selection_analysis(len(cols), to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3dcd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We round the values in the correlations, then remove masked values because we can't count them\n",
    "# Then we print a counter of the (rounded) correlations\n",
    "corrs_round = np.ma.array([np.round(c[0], 1) for c in corrs])\n",
    "corrs_round_not_masked = corrs_round[~corrs_round.mask].flatten()\n",
    "corrs_round_not_masked += 0. # This removes values that are rounded to -0.0, sets them to 0.0 (no minus)\n",
    "print(Counter(corrs_round_not_masked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330ea32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "54d668a7a2b723eaa45db485aeb9405c48b3714a12ae318aa481966bf3ee5079"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}